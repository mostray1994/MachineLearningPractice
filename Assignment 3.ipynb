{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h2>INFSCI 2915 Foundations- Machine Learning - Spring 2018 </h2>\n",
    "<h1 style=\"font-size: 250%;\">Assignment #3</h1>\n",
    "<h3>Issued Monday, 3/26/2018; Due Monday, 11:59pm, 4/09/2018</h3>\n",
    "<h3>Total points: 100 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type in your information in the double quotes\n",
    "firstName = \"Junxi\"\n",
    "lastName = \"Huang\"\n",
    "pittID = \"juh80\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #1.Linear Discriminant Analysis (LDA)and Quadratic Discriminant Analysis(QDA)      <br>[30 points]</h3> \n",
    " Writing a code is not required for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-1</h4>  <br>\n",
    "Assume we have K classes to be classified with one feature $(x)$. The prior probability of\n",
    "class $k$ is $ùúã_{k} = ùëÉ(ùëå = ùëò)$. Assume that the feature in class k has Gaussian distribution of\n",
    "mean $Œº_{k}$ and variance $œÉ^2 (ùí©(Œº,ùúé^{2}))$.The variance is the same for all classes. \n",
    "Prove that the Bayes‚Äô classifier (that chooses class k with largest $ùëÉ(ùëå = ùëò|ùë•))$ is equivalent to assigning an observation to the class for which the discriminant function $ùõø_{k}(x)$ is\n",
    "maximized, where \n",
    "\\begin{array} \\\\\n",
    "ùõø_{k}(x) = x\\frac{\\mu _{k}}{\\sigma ^{2}}- \\frac{\\mu_{2}^{k}}{2\\sigma ^{2}}+ log(\\pi _{k})\n",
    "\\end{array}\n",
    "<br> What is the name of this classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer(proof on picture):\n",
    "\n",
    "Linear Discriminant Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='homework3.jpg', width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-2</h4>  <br>\n",
    "Extend **Problem #1-1** to include **p** features. With features from each class drawn from a\n",
    "Gaussian distribution with mean vector $Œº_{k}$ and covariance matrix $Œ£_{k}$ (which is now\n",
    "different for each class). Find the discriminant function that maximizes **ùëÉ(ùëå = ùëò|ùë•)**. Is\n",
    "the relationship with the feature vector **x** linear?<br> What is this classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer1-2\n",
    "we get discriminate functions that are quadratic with x.\n",
    "name is:  \n",
    "Quadratic Discriminant Analysis(QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-3</h4>  <br>\n",
    "- Explain Bias-variance trade-off in choosing between LDA and QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer 1-3\n",
    "LDA is simple which has lower variance, but QDA has lower bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #2. Support Vector Machines  [20 points]</h3> \n",
    "Writing a code is not required for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-1 Answer the following quesionts </h4> \n",
    "\n",
    "- Describe limitations of using Maximal Margin Classifier \n",
    "- What is the difference between Support Vector Classifier and Maximal Margin Classifier\n",
    "- Explain Bias-variance trade-off that occurs when we choose large and small values for the Slack Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer 2.1:\n",
    "1.Maximal Margin Classifier is sensible to training data and classes may not be perfectly seperable by a hyperline\n",
    "\n",
    "2.Support vector classifier allows some training observations to be in the\n",
    "incorrect side of the hyperplane (decision boundary)\n",
    "\n",
    "3.if we choose large values for Slack Variables, it means the modle will more tolerance to validation with wide margin, which means large bias and small variance.                                                                                         \n",
    "if we choose large values for Slack Variables, it means the modle will less tolerance to validation with narrow margin, which means small bias and large variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-2</h4><br>\n",
    "How does the Radial Basis Function Kernel in SVM measure the similarity between a test point and a training example? Discuss the impact of choosing a large RBF parameter **ùõæ**\n",
    "on the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer 2.2:\n",
    "It computes the squared Euclidean distance between the observation point and training point. Small distance means more similarity, otherwise means less similarity.   \n",
    "A large RBF parameter ùõæ (small variance) may overfit as it becomes more local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #3 Classification Performance Evaluation and Cross validation [30 points]</h3> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-1</h4> <br>\n",
    "In a fraud detection system, a QDA classifier‚Äôs confusion matrix is found to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        |Predicted Class - Not fraud| Predicted Class - fraud|\n",
    "|:--:|:-------------------------------:|\n",
    "|Actual class ‚Äì Not fraud|1000|20|\n",
    "|Actual class ‚Äì fraud|30|5|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the overall error rate and the accuracy<br>\n",
    "- Evaluate the precision and the recall <br>\n",
    "- Is dataset balanced?\n",
    "- Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=1/7\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer 3-1\n",
    "1.The accuracy is :(1000+5)/(1000+5+20+30)=95.26%                                                                               \n",
    "  The error rate is (1-accuracy):(1-0.9526)=0.0474%\n",
    "  \n",
    "2.percision is true positive/all predict positive: 5/(20+5)=20%                                                                 \n",
    "  recall is true positive/ all actual positive: 5/(30+5)=14.29%\n",
    "  \n",
    "3.the dataset is not balanced, because samples of fraud class is much less than samples of not fraud class.\n",
    "\n",
    "4.The performance of this system on detect fraud class are bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-2</h4> <br>\n",
    "Assuming we want to lower the misclassification of fraud. How can you modify the classifier to do better classification using information from confusion matrix? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer 3-3\n",
    "1.Expand the data set. \n",
    "\n",
    "2.Resampling, for small data, apply oversampling, for big data, apply undersampling. \n",
    "\n",
    "3.Attempt to generate artificial data samples, like SMOTH method. \n",
    "\n",
    "4.try different classification algorithom, like decision tree. \n",
    "\n",
    "5.add punishment term to model. when the model classify the class of samll samples, add another punishment term model so that the model can more focus on the class of samll samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-3 Cross validation, SVM </h4> <br>\n",
    "In this exercise, we will use SVM  for breast cancer classification (malignant or benign).<br> \n",
    "Use a code below to download the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "print(dataset.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow steps to answer questions.\n",
    "- Scale the features with MinMaxScaler\n",
    "- Split breast cancer dataset into two, test dataset and train dataset\n",
    "- Find best SVM classifier model. Try values of C=[0.001, 0.1, 1, 10, 1000] and Gamma = [0.001, 0.1, 1, 10, 1000]. \n",
    "- Use 3-fold cross validation to find the best parameters (using all possible combinations of these values for C and gamma).\n",
    "- Report your parameters\n",
    "- Report accuracy, confusion matrix, precision, and recall (use Test dataset, SVM classifier model with best parameters)\n",
    "- Comment your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.79900000e+01   1.03800000e+01   1.22800000e+02   1.00100000e+03\n",
      "   1.18400000e-01   2.77600000e-01   3.00100000e-01   1.47100000e-01\n",
      "   2.41900000e-01   7.87100000e-02   1.09500000e+00   9.05300000e-01\n",
      "   8.58900000e+00   1.53400000e+02   6.39900000e-03   4.90400000e-02\n",
      "   5.37300000e-02   1.58700000e-02   3.00300000e-02   6.19300000e-03\n",
      "   2.53800000e+01   1.73300000e+01   1.84600000e+02   2.01900000e+03\n",
      "   1.62200000e-01   6.65600000e-01   7.11900000e-01   2.65400000e-01\n",
      "   4.60100000e-01   1.18900000e-01]\n",
      "[ 0.52103744  0.0226581   0.54598853  0.36373277  0.59375282  0.7920373\n",
      "  0.70313964  0.73111332  0.68636364  0.60551811  0.35614702  0.12046941\n",
      "  0.3690336   0.27381126  0.15929565  0.35139844  0.13568182  0.30062512\n",
      "  0.31164518  0.18304244  0.62077552  0.14152452  0.66831017  0.45069799\n",
      "  0.60113584  0.61929156  0.56861022  0.91202749  0.59846245  0.41886396]\n"
     ]
    }
   ],
   "source": [
    "data = dataset['data']\n",
    "target = dataset['target']\n",
    "print(data[0])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tain shape: (381, 30)\n",
      "X_test shape: (188, 30)\n",
      "y_train shape: (381,)\n",
      "y_test shape: (188,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=0)\n",
    "print('X_tain shape:',X_train.shape)\n",
    "print('X_test shape:',X_test.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('y_test shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= rbf    gamma and c: 0.001    score: 0.643617021277\n",
      "x= linear    gamma and c: 0.001    score: 0.643617021277\n",
      "x= poly    gamma and c: 0.001    score: 0.643617021277\n",
      "x= sigmoid    gamma and c: 0.001    score: 0.643617021277\n",
      "x= rbf    gamma and c: 0.1    score: 0.909574468085\n",
      "x= linear    gamma and c: 0.1    score: 0.941489361702\n",
      "x= poly    gamma and c: 0.1    score: 0.696808510638\n",
      "x= sigmoid    gamma and c: 0.1    score: 0.803191489362\n",
      "x= rbf    gamma and c: 1    score: 0.978723404255\n",
      "x= linear    gamma and c: 1    score: 0.973404255319\n",
      "x= poly    gamma and c: 1    score: 0.973404255319\n",
      "x= sigmoid    gamma and c: 1    score: 0.303191489362\n",
      "x= rbf    gamma and c: 10    score: 0.941489361702\n",
      "x= linear    gamma and c: 10    score: 0.973404255319\n",
      "x= poly    gamma and c: 10    score: 0.925531914894\n",
      "x= sigmoid    gamma and c: 10    score: 0.643617021277\n",
      "x= rbf    gamma and c: 1000    score: 0.643617021277\n",
      "x= linear    gamma and c: 1000    score: 0.941489361702\n",
      "x= poly    gamma and c: 1000    score: 0.925531914894\n",
      "x= sigmoid    gamma and c: 1000    score: 0.643617021277\n",
      "\n",
      " best\n",
      " max_method: rbf     gamma: 1    score: 0.978723404255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "max_method=''\n",
    "max_gamma=0.001\n",
    "max_score=0\n",
    "for i in [0.001, 0.1, 1, 10, 1000] :\n",
    "    for x in ['rbf', 'linear', 'poly','sigmoid']:\n",
    "        svmModel=SVC(kernel=x, gamma=i, C=i).fit(X_train,y_train)\n",
    "        score=svmModel.score(X_test,y_test)\n",
    "        if(score>max_score):\n",
    "            max_method=x\n",
    "            max_gamma=i\n",
    "            max_score=score\n",
    "        print('x=',x,'   gamma and c:',i,'   score:',score)\n",
    "print('\\n','best\\n','max_method:',max_method,'    gamma:',max_gamma,'   score:',max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= rbf    gamma: 0.001    c: 0.001    score: 0.627420402859\n",
      "x= rbf    gamma: 0.001    c: 0.1    score: 0.627420402859\n",
      "x= rbf    gamma: 0.001    c: 1    score: 0.627420402859\n",
      "x= rbf    gamma: 0.001    c: 10    score: 0.906943284136\n",
      "x= rbf    gamma: 0.001    c: 1000    score: 0.971883412234\n",
      "x= linear    gamma: 0.001    c: 0.001    score: 0.627420402859\n",
      "x= linear    gamma: 0.001    c: 0.1    score: 0.945558340295\n",
      "x= linear    gamma: 0.001    c: 1    score: 0.9718926947\n",
      "x= linear    gamma: 0.001    c: 10    score: 0.963092917479\n",
      "x= linear    gamma: 0.001    c: 1000    score: 0.94905782976\n",
      "x= poly    gamma: 0.001    c: 0.001    score: 0.627420402859\n",
      "x= poly    gamma: 0.001    c: 0.1    score: 0.627420402859\n",
      "x= poly    gamma: 0.001    c: 1    score: 0.627420402859\n",
      "x= poly    gamma: 0.001    c: 10    score: 0.627420402859\n",
      "x= poly    gamma: 0.001    c: 1000    score: 0.627420402859\n",
      "x= sigmoid    gamma: 0.001    c: 0.001    score: 0.627420402859\n",
      "x= sigmoid    gamma: 0.001    c: 0.1    score: 0.627420402859\n",
      "x= sigmoid    gamma: 0.001    c: 1    score: 0.627420402859\n",
      "x= sigmoid    gamma: 0.001    c: 10    score: 0.819075466444\n",
      "x= sigmoid    gamma: 0.001    c: 1000    score: 0.9718926947\n",
      "x= rbf    gamma: 0.1    c: 0.001    score: 0.627420402859\n",
      "x= rbf    gamma: 0.1    c: 0.1    score: 0.906934001671\n",
      "x= rbf    gamma: 0.1    c: 1    score: 0.95609393855\n",
      "x= rbf    gamma: 0.1    c: 10    score: 0.971883412234\n",
      "x= rbf    gamma: 0.1    c: 1000    score: 0.961338531514\n",
      "x= linear    gamma: 0.1    c: 0.001    score: 0.627420402859\n",
      "x= linear    gamma: 0.1    c: 0.1    score: 0.945558340295\n",
      "x= linear    gamma: 0.1    c: 1    score: 0.9718926947\n",
      "x= linear    gamma: 0.1    c: 10    score: 0.963092917479\n",
      "x= linear    gamma: 0.1    c: 1000    score: 0.94905782976\n",
      "x= poly    gamma: 0.1    c: 0.001    score: 0.627420402859\n",
      "x= poly    gamma: 0.1    c: 0.1    score: 0.676654599462\n",
      "x= poly    gamma: 0.1    c: 1    score: 0.89462545252\n",
      "x= poly    gamma: 0.1    c: 10    score: 0.945558340295\n",
      "x= poly    gamma: 0.1    c: 1000    score: 0.9718926947\n",
      "x= sigmoid    gamma: 0.1    c: 0.001    score: 0.627420402859\n",
      "x= sigmoid    gamma: 0.1    c: 0.1    score: 0.792722547109\n",
      "x= sigmoid    gamma: 0.1    c: 1    score: 0.94731272626\n",
      "x= sigmoid    gamma: 0.1    c: 10    score: 0.970129026269\n",
      "x= sigmoid    gamma: 0.1    c: 1000    score: 0.954311705189\n",
      "x= rbf    gamma: 1    c: 0.001    score: 0.627420402859\n",
      "x= rbf    gamma: 1    c: 0.1    score: 0.94380395433\n",
      "x= rbf    gamma: 1    c: 1    score: 0.97716513506\n",
      "x= rbf    gamma: 1    c: 10    score: 0.97540146663\n",
      "x= rbf    gamma: 1    c: 1000    score: 0.956047526223\n",
      "x= linear    gamma: 1    c: 0.001    score: 0.627420402859\n",
      "x= linear    gamma: 1    c: 0.1    score: 0.945558340295\n",
      "x= linear    gamma: 1    c: 1    score: 0.9718926947\n",
      "x= linear    gamma: 1    c: 10    score: 0.963092917479\n",
      "x= linear    gamma: 1    c: 1000    score: 0.94905782976\n",
      "x= poly    gamma: 1    c: 0.001    score: 0.89462545252\n",
      "x= poly    gamma: 1    c: 0.1    score: 0.973647080665\n",
      "x= poly    gamma: 1    c: 1    score: 0.9718926947\n",
      "x= poly    gamma: 1    c: 10    score: 0.966629536805\n",
      "x= poly    gamma: 1    c: 1000    score: 0.95433027012\n",
      "x= sigmoid    gamma: 1    c: 0.001    score: 0.627420402859\n",
      "x= sigmoid    gamma: 1    c: 0.1    score: 0.562470992296\n",
      "x= sigmoid    gamma: 1    c: 1    score: 0.302441288406\n",
      "x= sigmoid    gamma: 1    c: 10    score: 0.279587858535\n",
      "x= sigmoid    gamma: 1    c: 1000    score: 0.276079086605\n",
      "x= rbf    gamma: 10    c: 0.001    score: 0.627420402859\n",
      "x= rbf    gamma: 10    c: 0.1    score: 0.662610229277\n",
      "x= rbf    gamma: 10    c: 1    score: 0.941984591107\n",
      "x= rbf    gamma: 10    c: 10    score: 0.938475819178\n",
      "x= rbf    gamma: 10    c: 1000    score: 0.938475819178\n",
      "x= linear    gamma: 10    c: 0.001    score: 0.627420402859\n",
      "x= linear    gamma: 10    c: 0.1    score: 0.945558340295\n",
      "x= linear    gamma: 10    c: 1    score: 0.9718926947\n",
      "x= linear    gamma: 10    c: 10    score: 0.963092917479\n",
      "x= linear    gamma: 10    c: 1000    score: 0.94905782976\n",
      "x= poly    gamma: 10    c: 0.001    score: 0.9718926947\n",
      "x= poly    gamma: 10    c: 0.1    score: 0.95433027012\n",
      "x= poly    gamma: 10    c: 1    score: 0.95433027012\n",
      "x= poly    gamma: 10    c: 10    score: 0.95433027012\n",
      "x= poly    gamma: 10    c: 1000    score: 0.95433027012\n",
      "x= sigmoid    gamma: 10    c: 0.001    score: 0.627420402859\n",
      "x= sigmoid    gamma: 10    c: 0.1    score: 0.627420402859\n",
      "x= sigmoid    gamma: 10    c: 1    score: 0.627420402859\n",
      "x= sigmoid    gamma: 10    c: 10    score: 0.627420402859\n",
      "x= sigmoid    gamma: 10    c: 1000    score: 0.5905504502\n",
      "x= rbf    gamma: 1000    c: 0.001    score: 0.627420402859\n",
      "x= rbf    gamma: 1000    c: 0.1    score: 0.627420402859\n",
      "x= rbf    gamma: 1000    c: 1    score: 0.627420402859\n",
      "x= rbf    gamma: 1000    c: 10    score: 0.627420402859\n",
      "x= rbf    gamma: 1000    c: 1000    score: 0.627420402859\n",
      "x= linear    gamma: 1000    c: 0.001    score: 0.627420402859\n",
      "x= linear    gamma: 1000    c: 0.1    score: 0.945558340295\n",
      "x= linear    gamma: 1000    c: 1    score: 0.9718926947\n",
      "x= linear    gamma: 1000    c: 10    score: 0.963092917479\n",
      "x= linear    gamma: 1000    c: 1000    score: 0.94905782976\n",
      "x= poly    gamma: 1000    c: 0.001    score: 0.95433027012\n",
      "x= poly    gamma: 1000    c: 0.1    score: 0.95433027012\n",
      "x= poly    gamma: 1000    c: 1    score: 0.95433027012\n",
      "x= poly    gamma: 1000    c: 10    score: 0.95433027012\n",
      "x= poly    gamma: 1000    c: 1000    score: 0.95433027012\n",
      "x= sigmoid    gamma: 1000    c: 0.001    score: 0.627420402859\n",
      "x= sigmoid    gamma: 1000    c: 0.1    score: 0.627420402859\n",
      "x= sigmoid    gamma: 1000    c: 1    score: 0.627420402859\n",
      "x= sigmoid    gamma: 1000    c: 10    score: 0.627420402859\n",
      "x= sigmoid    gamma: 1000    c: 1000    score: 0.627420402859\n",
      "\n",
      " best\n",
      " max_method: rbf     gamma: 1       c: 1    score: 0.97716513506\n"
     ]
    }
   ],
   "source": [
    "#Use 3-fold cross validation to find the best parameters (using all possible combinations of these values for C and gamma).\n",
    "#Report your parameters\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "max_method=''\n",
    "max_gamma=0.001\n",
    "max_score=0\n",
    "for i in [0.001, 0.1, 1, 10, 1000] :\n",
    "    for x in ['rbf', 'linear', 'poly','sigmoid']:\n",
    "       for j in [0.001, 0.1, 1, 10, 1000] :\n",
    "            svmModel=SVC(kernel=x, gamma=i, C=j)\n",
    "            score = cross_val_score(svmModel, data, target, cv=3)\n",
    "            if(score.mean()>max_score):\n",
    "                max_method=x\n",
    "                max_gamma=i\n",
    "                max_c=j\n",
    "                max_score=score.mean()\n",
    "            print('x=',x,'   gamma:',i,'   c:',j,'   score:',score.mean())\n",
    "print('\\n','best\\n','max_method:',max_method,'    gamma:',max_gamma,'      c:',max_c,'   score:',max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.978723404255\n",
      "confusion matrix: \n",
      " [[64, 3], [1, 120]]\n",
      "recall:  0.99173553719\n",
      "precision:  0.975609756098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "svcModel = SVC(C = 1.0,gamma = 1.0).fit(X_train,y_train)\n",
    "prediction = svcModel.predict(X_test)\n",
    "score = svcModel.score(X_test, y_test)\n",
    "print(\"score: \",score)\n",
    "cm = confusion_matrix(y_test, prediction).tolist()\n",
    "print(\"confusion matrix: \\n\",cm)\n",
    "recall = recall_score(y_test, prediction)\n",
    "print(\"recall: \",recall)\n",
    "precision = precision_score(y_test, prediction)\n",
    "print(\"precision: \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3-3:\n",
    "the best parameter is: max_method: rbf     gamma: 1       c: 1    score: 0.97716513506"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-4 Cross validation, Logistic regression </h4>\n",
    "\n",
    "- Repeat **Problem #3-3** but instead of SVM find best Logistic regression classifier model. Try values of C= [0.001,0.01,1,10,100, 1000] and Penalty = [\"l1\",\"l2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty is l1:\n",
      "0.372579597141\n",
      "0.372579597141\n",
      "0.961338531514\n",
      "0.961338531514\n",
      "0.959593428014\n",
      "0.949067112225\n",
      "penalty is l2:\n",
      "0.688981713543\n",
      "0.864800891117\n",
      "0.954339552585\n",
      "0.9718926947\n",
      "0.971883412234\n",
      "0.963102199944\n",
      "\n",
      " best\n",
      " penalty: 12       c: 10    score: 0.9718926947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "C= [0.001,0.01,1,10,100, 1000]\n",
    "\n",
    "Penalty='11'\n",
    "max_c=0.001\n",
    "max_score=0\n",
    "\n",
    "print(\"penalty is l1:\")\n",
    "for i in C :\n",
    "    log = LogisticRegression(penalty = 'l1',C = i)\n",
    "    score = cross_val_score(log, data, target, cv=3)\n",
    "    print(score.mean())\n",
    "    if(score.mean()>max_score):\n",
    "            Penalty='11'\n",
    "            max_c=i\n",
    "            max_score=score.mean()\n",
    "\n",
    "print(\"penalty is l2:\")\n",
    "for i in C :\n",
    "    log = LogisticRegression(penalty = 'l2',C = i)\n",
    "    score = cross_val_score(log, data, target, cv=3)\n",
    "    print(score.mean())\n",
    "    if(score.mean()>max_score):\n",
    "            Penalty='12'\n",
    "            max_c=i\n",
    "            max_score=score.mean()\n",
    "            \n",
    "print('\\n','best\\n','penalty:',Penalty,'      c:',max_c,'   score:',max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.973404255319\n",
      "confusion matrix: \n",
      " [[63, 4], [1, 120]]\n",
      "recall:  0.99173553719\n",
      "precision:  0.967741935484\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression(penalty = 'l2',C =10 ).fit(X_train,y_train)\n",
    "prediction = log.predict(X_test)\n",
    "score = log.score(X_test, y_test)\n",
    "print(\"score: \",score)\n",
    "cm = confusion_matrix(y_test, prediction).tolist()\n",
    "print(\"confusion matrix: \\n\",cm)\n",
    "recall = recall_score(y_test, prediction)\n",
    "print(\"recall: \",recall)\n",
    "precision = precision_score(y_test, prediction)\n",
    "print(\"precision: \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #4  Neural Networks [20 points]</h3> \n",
    "In this exercise we will clasify Iris species (Setosa, Versicolor, Virginica) using Neural Networks.<br>\n",
    "Use a code below to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset=load_iris()\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow steps to answer questions.\n",
    "- Scale the feautures with MinMaxScaler, then use sklearn MLPClassifier. \n",
    "- Build a model that has two hidden layers, the first layer has 10 neurons and second layer has 5 neurons. \n",
    "- Use 'relu' activation function, and set the regularization parameter alpha=0.5. \n",
    "- Set the random_state=0 for splitting and building all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-1 </h4>\n",
    "- Use gradient descent to solve the optimization  problem (i.e. get the weights), and choose random_state=0 (which corresponds to a particular initializationo of weight values). \n",
    "- Report accuracy, confusion matrix, precision, and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scale:  [  1.79900000e+01   1.03800000e+01   1.22800000e+02   1.00100000e+03\n",
      "   1.18400000e-01   2.77600000e-01   3.00100000e-01   1.47100000e-01\n",
      "   2.41900000e-01   7.87100000e-02   1.09500000e+00   9.05300000e-01\n",
      "   8.58900000e+00   1.53400000e+02   6.39900000e-03   4.90400000e-02\n",
      "   5.37300000e-02   1.58700000e-02   3.00300000e-02   6.19300000e-03\n",
      "   2.53800000e+01   1.73300000e+01   1.84600000e+02   2.01900000e+03\n",
      "   1.62200000e-01   6.65600000e-01   7.11900000e-01   2.65400000e-01\n",
      "   4.60100000e-01   1.18900000e-01]\n",
      "after scale [ 0.52103744  0.0226581   0.54598853  0.36373277  0.59375282  0.7920373\n",
      "  0.70313964  0.73111332  0.68636364  0.60551811  0.35614702  0.12046941\n",
      "  0.3690336   0.27381126  0.15929565  0.35139844  0.13568182  0.30062512\n",
      "  0.31164518  0.18304244  0.62077552  0.14152452  0.66831017  0.45069799\n",
      "  0.60113584  0.61929156  0.56861022  0.91202749  0.59846245  0.41886396]\n"
     ]
    }
   ],
   "source": [
    "#Write your answer here\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = dataset['data']\n",
    "target = dataset['target']\n",
    "print('before scale: ', data[0])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "print('after scale',data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.629370629371\n",
      "confusion matrix: \n",
      " [[0, 53], [0, 90]]\n",
      "recall:  0.5\n",
      "precision:  0.314685314685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLPmodel = MLPClassifier(solver='sgd', activation='relu', random_state=0, hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLPmodel.fit(X_train,y_train)\n",
    "prediction = MLPmodel.predict(X_test)\n",
    "score = accuracy_score(y_test,prediction)\n",
    "print(\"accuracy score: \",score)\n",
    "cm = confusion_matrix(y_test, prediction).tolist()\n",
    "print(\"confusion matrix: \\n\",cm)\n",
    "recall = recall_score(y_test, prediction, average='macro')\n",
    "print(\"recall: \",recall)\n",
    "precision = precision_score(y_test, prediction, average='macro')\n",
    "print(\"precision: \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-2 </h4>\n",
    "- Repeat **Problem #4-1** but with a model that use random_state=10 to initialize the weights. \n",
    "- Report accuracy, confusion matrix, precision, and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958041958042\n"
     ]
    }
   ],
   "source": [
    "#Write your answer here\n",
    "MLPmodel = MLPClassifier(solver='sgd', activation='relu', random_state=0, max_iter = 4000,hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLPmodel.fit(X_train,y_train)\n",
    "prediction = MLPmodel.predict(X_test)\n",
    "score = accuracy_score(y_test,prediction)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-3 </h4>\n",
    "- Repeat **Problem #4-2** but with model that use L-BFGS (a numerical quasi-Newton method) instead of stochastic gradient descent to find the weights.\n",
    "- Report accuracy, confusion matrix, precision, and recall\n",
    "- Comment on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.944055944056\n",
      "confusion matrix: \n",
      " [[50, 3], [5, 85]]\n",
      "recall:  0.94392033543\n",
      "precision:  0.9375\n"
     ]
    }
   ],
   "source": [
    "#Write your answer here\n",
    "MLPmodel = MLPClassifier(solver='lbfgs', activation='relu', random_state=10, hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLPmodel.fit(X_train,y_train)\n",
    "prediction = MLPmodel.predict(X_test)\n",
    "score = accuracy_score(y_test,prediction)\n",
    "print(\"accuracy score: \",score)\n",
    "cm = confusion_matrix(y_test, prediction).tolist()\n",
    "print(\"confusion matrix: \\n\",cm)\n",
    "recall = recall_score(y_test, prediction, average='macro')\n",
    "print(\"recall: \",recall)\n",
    "precision = precision_score(y_test, prediction, average='macro')\n",
    "print(\"precision: \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer 4-3\n",
    "       \n",
    "lbfgs is more suitable for small data sets.   \n",
    "also, add more hidden layer doesn't help much.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
