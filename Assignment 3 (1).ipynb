{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h2>INFSCI 2915 Foundations- Machine Learning - Spring 2018 </h2>\n",
    "<h1 style=\"font-size: 250%;\">Assignment #3</h1>\n",
    "<h3>Issued Monday, 3/26/2018; Due Monday, 11:59pm, 4/09/2018</h3>\n",
    "<h3>Total points: 100 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type in your information in the double quotes\n",
    "firstName = \"Echhit\"\n",
    "lastName = \"Joshi\"\n",
    "pittID = \"EBJ7@pitt.edu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #1.Linear Discriminant Analysis (LDA)and Quadratic Discriminant Analysis(QDA)      <br>[30 points]</h3> \n",
    " Writing a code is not required for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-1</h4>  <br>\n",
    "Assume we have K classes to be classified with one feature $(x)$. The prior probability of\n",
    "class $k$ is $ùúã_{k} = ùëÉ(ùëå = ùëò)$. Assume that the feature in class k has Gaussian distribution of\n",
    "mean $Œº_{k}$ and variance $œÉ^2 (ùí©(Œº,ùúé^{2}))$.The variance is the same for all classes. \n",
    "Prove that the Bayes‚Äô classifier (that chooses class k with largest $ùëÉ(ùëå = ùëò|ùë•))$ is equivalent to assigning an observation to the class for which the discriminant function $ùõø_{k}(x)$ is\n",
    "maximized, where \n",
    "\\begin{array} \\\\\n",
    "ùõø_{k}(x) = x\\frac{\\mu _{k}}{\\sigma ^{2}}- \\frac{\\mu_{2}^{k}}{2\\sigma ^{2}}+ log(\\pi _{k})\n",
    "\\end{array}\n",
    "<br> What is the name of this classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANSWER(REFER ATTACHED PICTURE FOR PROOF)\n",
    "    \n",
    "    The name of the Classifier is the Bayes Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-2</h4>  <br>\n",
    "Extend **Problem #1-1** to include **p** features. With features from each class drawn from a\n",
    "Gaussian distribution with mean vector $Œº_{k}$ and covariance matrix $Œ£_{k}$ (which is now\n",
    "different for each class). Find the discriminant function that maximizes **ùëÉ(ùëå = ùëò|ùë•)**. Is\n",
    "the relationship with the feature vector **x** linear?<br> What is this classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANSWER(REFER ATTACHED PICTURE FOR PROOF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-3</h4>  <br>\n",
    "- Explain Bias-variance trade-off in choosing between LDA and QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANSWER:\n",
    "\n",
    "    LDA is the simpler model between the two. It might have high bias as it is less complex compared to QDA. Since it only has 1 class to predict, It just needs to estimate a single covariance matrix. For QDA, covariance matrix is estimated for each class. Each matrix has (p*(p+1))/2 parameters. This makes QDA very complex and therefore there might be a problem of overfitting if the training dataset is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #2. Support Vector Machines  [20 points]</h3> \n",
    "Writing a code is not required for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-1 Answer the following quesionts </h4> \n",
    "\n",
    "- Describe limitations of using Maximal Margin Classifier \n",
    "- What is the difference between Support Vector Classifier and Maximal Margin Classifier\n",
    "- Explain Bias-variance trade-off that occurs when we choose large and small values for the Slack Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANSWER\n",
    "\n",
    "1) The Maximal Margin Classifier is very sensitive to the training set that it is derived from. A small change in the training data could result in drastically different Margin which may not perform very well on the test data. Another limitation is that the data may actually not be seperable using a Maximal Margin Classifier and therefore the classifier might result in high errors.\n",
    "\n",
    "2) The main difference between Support Vector Classifier and Maximal Margin Classifier is that Maximal Margin Classifier is a fixed boundary and does not account for points that are on the wrong side of the boundary. Support Vector Classifier uses slack variables to determine training points that are on the wrong side of the decision boundary and accounts for the positions of the data points within a range of the wrong side of the decision boundary.\n",
    "\n",
    "3) When Slack Variables are large, the Classifier is more tolerable to violation by the points being on the wrong side of the boundary. This in term creates a large margin and therefore there is high bias, with low variance. Conversely, when Slack Variables are small, the Classifier is more strict and does not tolerate as much violation. This could result in overfitting and the Classifier only performs well on the training data. This creates a high variance with low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-2</h4><br>\n",
    "How does the Radial Basis Function Kernel in SVM measure the similarity between a test point and a training example? Discuss the impact of choosing a large RBF parameter **ùõæ**\n",
    "on the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANSWER\n",
    "\n",
    "1) As Radial Basis Function Kernel in SVM is a Gaussian-like measure, the kernel measures the similarity between a test point and a training example by computing the squared Euclidean distance between the test and training example. \n",
    "    When a large RBF parameter ùõæ is chosen, the spread of the distribution decreases which in turn results in a high variance and low bias. When test points are compared to the training example, it might not be able to accurate predict the test point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #3 Classification Performance Evaluation and Cross validation [30 points]</h3> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-1</h4> <br>\n",
    "In a fraud detection system, a QDA classifier‚Äôs confusion matrix is found to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        |Predicted Class - Not fraud| Predicted Class - fraud|\n",
    "|:--:|:-------------------------------:|\n",
    "|Actual class ‚Äì Not fraud|1000|20|\n",
    "|Actual class ‚Äì fraud|30|5|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the overall error rate and the accuracy<br>\n",
    "- Evaluate the precision and the recall <br>\n",
    "- Is dataset balanced?\n",
    "- Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANSWER\n",
    "\n",
    "1) Overall error rate = (total error)/(total observations) = (30+20)/1055 = 0.0474\n",
    "   Accuracy of Confusion Matrix = (True Fraud + True Not Fraud)/ (total observations) = (1000+5)/1055 = 0.953\n",
    "   \n",
    "2) Precision = (True Fraud)/(Total Predicted Fraud) = 5/(20+5) = .2\n",
    "   Recall = (True Fraud)/(Actual Fraud) = 5/(30+5) = .143\n",
    "\n",
    "3) The dataset is not balanced as there is a skewed class. There is not enough training samples for the \"Fraud\" class(25) compared to the \"Not Fraud\" class(1030). \n",
    "\n",
    "4) We can see that the Accuracy is very high while there is no sufficent evidence for the \"Fraud\" class to be correctly predicted given the observation count. The Precision and Recall scores are very low suggesting that the dataset is skewed. The Precision is only .2 which means that only 20% of the total predicted \"Fraud\" is correct. While the Recall is only .143 which means that only 14.3% of datapoints were accurately detected as \"Fraud\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-2</h4> <br>\n",
    "Assuming we want to lower the misclassification of fraud. How can you modify the classifier to do better classification using information from confusion matrix? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANSWER\n",
    "\n",
    "    We could modify the classifier to do better classification by significantly lowering the threshold for the \"Fraud\" class. Since there are only two classes, the default threshold for predicting a class given the features would be 0.5, Setting a value less than 0.5 would theoretically increase the prediction accuracy given the skewed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-3 Cross validation, SVM </h4> <br>\n",
    "In this exercise, we will use SVM  for breast cancer classification (malignant or benign).<br> \n",
    "Use a code below to download the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "dataset = load_breast_cancer()\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow steps to answer questions.\n",
    "- Scale the features with MinMaxScaler\n",
    "- Split breast cancer dataset into two, test dataset and train dataset\n",
    "- Find best SVM classifier model. Try values of C=[0.001, 0.1, 1, 10, 1000] and Gamma = [0.001, 0.1, 1, 10, 1000]. \n",
    "- Use 3-fold cross validation to find the best parameters (using all possible combinations of these values for C and gamma).\n",
    "- Report your parameters\n",
    "- Report accuracy, confusion matrix, precision, and recall (use Test dataset, SVM classifier model with best parameters)\n",
    "- Comment your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>...</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.13</td>\n",
       "      <td>19.29</td>\n",
       "      <td>91.97</td>\n",
       "      <td>654.89</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>16.27</td>\n",
       "      <td>25.68</td>\n",
       "      <td>107.26</td>\n",
       "      <td>880.58</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.52</td>\n",
       "      <td>4.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>351.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.15</td>\n",
       "      <td>33.60</td>\n",
       "      <td>569.36</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.98</td>\n",
       "      <td>9.71</td>\n",
       "      <td>43.79</td>\n",
       "      <td>143.50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.93</td>\n",
       "      <td>12.02</td>\n",
       "      <td>50.41</td>\n",
       "      <td>185.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.70</td>\n",
       "      <td>16.17</td>\n",
       "      <td>75.17</td>\n",
       "      <td>420.30</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01</td>\n",
       "      <td>21.08</td>\n",
       "      <td>84.11</td>\n",
       "      <td>515.30</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.37</td>\n",
       "      <td>18.84</td>\n",
       "      <td>86.24</td>\n",
       "      <td>551.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>14.97</td>\n",
       "      <td>25.41</td>\n",
       "      <td>97.66</td>\n",
       "      <td>686.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.78</td>\n",
       "      <td>21.80</td>\n",
       "      <td>104.10</td>\n",
       "      <td>782.70</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>18.79</td>\n",
       "      <td>29.72</td>\n",
       "      <td>125.40</td>\n",
       "      <td>1084.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.11</td>\n",
       "      <td>39.28</td>\n",
       "      <td>188.50</td>\n",
       "      <td>2501.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>36.04</td>\n",
       "      <td>49.54</td>\n",
       "      <td>251.20</td>\n",
       "      <td>4254.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "count       569.00        569.00          569.00     569.00           569.00   \n",
       "mean         14.13         19.29           91.97     654.89             0.10   \n",
       "std           3.52          4.30           24.30     351.91             0.01   \n",
       "min           6.98          9.71           43.79     143.50             0.05   \n",
       "25%          11.70         16.17           75.17     420.30             0.09   \n",
       "50%          13.37         18.84           86.24     551.10             0.10   \n",
       "75%          15.78         21.80          104.10     782.70             0.11   \n",
       "max          28.11         39.28          188.50    2501.00             0.16   \n",
       "\n",
       "       mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "count            569.00          569.00               569.00         569.00   \n",
       "mean               0.10            0.09                 0.05           0.18   \n",
       "std                0.05            0.08                 0.04           0.03   \n",
       "min                0.02            0.00                 0.00           0.11   \n",
       "25%                0.06            0.03                 0.02           0.16   \n",
       "50%                0.09            0.06                 0.03           0.18   \n",
       "75%                0.13            0.13                 0.07           0.20   \n",
       "max                0.35            0.43                 0.20           0.30   \n",
       "\n",
       "       mean fractal dimension           ...             worst radius  \\\n",
       "count                  569.00           ...                   569.00   \n",
       "mean                     0.06           ...                    16.27   \n",
       "std                      0.01           ...                     4.83   \n",
       "min                      0.05           ...                     7.93   \n",
       "25%                      0.06           ...                    13.01   \n",
       "50%                      0.06           ...                    14.97   \n",
       "75%                      0.07           ...                    18.79   \n",
       "max                      0.10           ...                    36.04   \n",
       "\n",
       "       worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "count         569.00           569.00      569.00            569.00   \n",
       "mean           25.68           107.26      880.58              0.13   \n",
       "std             6.15            33.60      569.36              0.02   \n",
       "min            12.02            50.41      185.20              0.07   \n",
       "25%            21.08            84.11      515.30              0.12   \n",
       "50%            25.41            97.66      686.50              0.13   \n",
       "75%            29.72           125.40     1084.00              0.15   \n",
       "max            49.54           251.20     4254.00              0.22   \n",
       "\n",
       "       worst compactness  worst concavity  worst concave points  \\\n",
       "count             569.00           569.00                569.00   \n",
       "mean                0.25             0.27                  0.11   \n",
       "std                 0.16             0.21                  0.07   \n",
       "min                 0.03             0.00                  0.00   \n",
       "25%                 0.15             0.11                  0.06   \n",
       "50%                 0.21             0.23                  0.10   \n",
       "75%                 0.34             0.38                  0.16   \n",
       "max                 1.06             1.25                  0.29   \n",
       "\n",
       "       worst symmetry  worst fractal dimension  \n",
       "count          569.00                   569.00  \n",
       "mean             0.29                     0.08  \n",
       "std              0.06                     0.02  \n",
       "min              0.16                     0.06  \n",
       "25%              0.25                     0.07  \n",
       "50%              0.28                     0.08  \n",
       "75%              0.32                     0.09  \n",
       "max              0.66                     0.21  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write your answer here\n",
    "dataFrame1=pd.DataFrame(dataset.data,columns=dataset.feature_names)\n",
    "scaler=preprocessing.MinMaxScaler()\n",
    "scaler.fit(dataset['data'])\n",
    "X_transformed=scaler.transform(dataset['data'])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_transformed,dataset['target'],random_state=0)\n",
    "dataFrame1.describe(include='all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy when C and G are  0.001  is :  0.629370629371\n",
      "The Accuracy when C and G are  0.1  is :  0.937062937063\n",
      "The Accuracy when C and G are  1  is :  0.972027972028\n",
      "The Accuracy when C and G are  10  is :  0.951048951049\n",
      "The Accuracy when C and G are  1000  is :  0.629370629371\n"
     ]
    }
   ],
   "source": [
    "#best SVM Classifier Model:\n",
    "a = [0.001, 0.1, 1, 10, 1000]\n",
    "for i in a:\n",
    "    svmModel=SVC(kernel='rbf', gamma=i, C=i)\n",
    "    svmModel.fit(X_train,Y_train)\n",
    "    print(\"The Accuracy when C and G are \", i ,\" is : \", svmModel.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Scores using 3-fold validation when C is: 0.001 and G is  0.001  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 0.001 and G is  0.1  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 0.001 and G is  1  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 0.001 and G is  10  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 0.001 and G is  1000  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 0.1 and G is  0.001  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 0.1 and G is  0.1  is :  0.87558685446\n",
      "The Scores using 3-fold validation when C is: 0.1 and G is  1  is :  0.943661971831\n",
      "The Scores using 3-fold validation when C is: 0.1 and G is  10  is :  0.629107981221\n",
      "The Scores using 3-fold validation when C is: 0.1 and G is  1000  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 1 and G is  0.001  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 1 and G is  0.1  is :  0.953051643192\n",
      "The Scores using 3-fold validation when C is: 1 and G is  1  is :  0.978873239437\n",
      "The Scores using 3-fold validation when C is: 1 and G is  10  is :  0.934272300469\n",
      "The Scores using 3-fold validation when C is: 1 and G is  1000  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 10 and G is  0.001  is :  0.880281690141\n",
      "The Scores using 3-fold validation when C is: 10 and G is  0.1  is :  0.978873239437\n",
      "The Scores using 3-fold validation when C is: 10 and G is  1  is :  0.978873239437\n",
      "The Scores using 3-fold validation when C is: 10 and G is  10  is :  0.924882629108\n",
      "The Scores using 3-fold validation when C is: 10 and G is  1000  is :  0.62676056338\n",
      "The Scores using 3-fold validation when C is: 1000 and G is  0.001  is :  0.976525821596\n",
      "The Scores using 3-fold validation when C is: 1000 and G is  0.1  is :  0.962441314554\n",
      "The Scores using 3-fold validation when C is: 1000 and G is  1  is :  0.93896713615\n",
      "The Scores using 3-fold validation when C is: 1000 and G is  10  is :  0.924882629108\n",
      "The Scores using 3-fold validation when C is: 1000 and G is  1000  is :  0.62676056338\n"
     ]
    }
   ],
   "source": [
    "#3 fold Cross validation to find best parameters:\n",
    "for i in a:\n",
    "    for x in range(0,len(a)):\n",
    "        svmModel=SVC(kernel='rbf', gamma=a[x], C=i)\n",
    "        svmModel.fit(X_train,Y_train)\n",
    "        scores = cross_val_score(svmModel, X_train, Y_train, cv=3)\n",
    "        score = np.mean(scores)\n",
    "        print(\"The Scores using 3-fold validation when C is:\",i, \"and G is \", a[x] ,\" is : \",score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER\n",
    "\n",
    "    The best parameters using the 3-fold Cross Validation are (C=1,G=1), (C=10,G=0.1), or (C=10,G=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-4 Cross validation, Logistic regression </h4>\n",
    "\n",
    "- Repeat **Problem #3-3** but instead of SVM find best Logistic regression classifier model. Try values of C= [0.001,0.01,1,10,100, 1000] and Penalty = [\"l1\",\"l2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score on the validation is:  0.981308411215\n",
      "The best parameter on the validation is:  10\n",
      "The test score with best Parameters is:  0.965034965035\n"
     ]
    }
   ],
   "source": [
    "#Using Logistic Regression Classifier Model Instead of SVM\n",
    "#Second Split for seperating Validation Set:\n",
    "X_trainV, X_testV, Y_trainV, Y_testV = train_test_split(X_train,Y_train,random_state=0)\n",
    "b = ['l1','l2']\n",
    "best_score = 0\n",
    "for x in range(0,1):\n",
    "    penalty = b[x]\n",
    "    for i in a:\n",
    "        logRegModel = LogisticRegression(penalty=penalty,C=i).fit(X_trainV,Y_trainV)\n",
    "        score1 = logRegModel.score(X_testV,Y_testV)\n",
    "        if score1>best_score:\n",
    "            best_score = score1\n",
    "            best_parameter = i\n",
    "NewlogRegModel = LogisticRegression(C=best_parameters).fit(X_train,Y_train)\n",
    "test_score = NewlogRegModel.score(X_test,Y_test)\n",
    "print(\"The best score on the validation is: \",best_score)\n",
    "print(\"The best parameter on the validation is: \",best_parameter)\n",
    "print(\"The test score with best Parameters is: \",test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #4  Neural Networks [20 points]</h3> \n",
    "In this exercise we will clasify Iris species (Setosa, Versicolor, Virginica) using Neural Networks.<br>\n",
    "Use a code below to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "dataset2=load_iris()\n",
    "print(dataset2.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow steps to answer questions.\n",
    "- Scale the feautures with MinMaxScaler, then use sklearn MLPClassifier. \n",
    "- Build a model that has two hidden layers, the first layer has 10 neurons and second layer has 5 neurons. \n",
    "- Use 'relu' activation function, and set the regularization parameter alpha=0.5. \n",
    "- Set the random_state=0 for splitting and building all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-1 </h4>\n",
    "- Use gradient descent to solve the optimization  problem (i.e. get the weights), and choose random_state=0 (which corresponds to a particular initializationo of weight values). \n",
    "- Report accuracy, confusion matrix, precision, and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the MLP Classifier is:  0.973684210526\n",
      "The Confusion Matrix is: \n",
      " [[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n",
      "The Precision score for MLP model is:  0.966666666667\n",
      "The Recall score for MLP model is:  0.979166666667\n"
     ]
    }
   ],
   "source": [
    "#Scaling features and then solving the optimization problem using gradient descent(Optimization only converged with max iteration of 3000)\n",
    "scaler=preprocessing.MinMaxScaler()\n",
    "scaler.fit(dataset2['data'])\n",
    "X_train_scaled = scaler.transform(dataset2['data'])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train_scaled,dataset2['target'],random_state=0)\n",
    "MLPmodel=MLPClassifier(solver = 'sgd', activation = 'relu', random_state = 0,max_iter=3000, hidden_layer_sizes=[10,5], alpha = 0.5)\n",
    "MLPmodel.fit(X_train,Y_train)\n",
    "\n",
    "#Accuracy, Confusion Matrix, Precision, and Recall\n",
    "print(\"The Accuracy of the MLP Classifier is: \", MLPmodel.score(X_test,Y_test))\n",
    "PredictedOutput=MLPmodel.predict(X_test)\n",
    "confusion=confusion_matrix(Y_test,PredictedOutput)\n",
    "print(\"The Confusion Matrix is: \\n\",confusion)\n",
    "print(\"The Precision score for MLP model is: \", precision_score(Y_test,PredictedOutput,average='macro'))\n",
    "print(\"The Recall score for MLP model is: \", recall_score(Y_test,PredictedOutput,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-2 </h4>\n",
    "- Repeat **Problem #4-1** but with a model that use random_state=10 to initialize the weights. \n",
    "- Report accuracy, confusion matrix, precision, and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the MLP Classifier is:  0.947368421053\n",
      "The Confusion Matrix is: \n",
      " [[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n",
      "The Precision score for MLP model is:  0.966666666667\n",
      "The Recall score for MLP model is:  0.979166666667\n"
     ]
    }
   ],
   "source": [
    "#Initializing the weight by setting random_state to 10\n",
    "MLPmodel2=MLPClassifier(solver = 'sgd', activation = 'relu', random_state = 10,max_iter=3000, hidden_layer_sizes=[10,5], alpha = 0.5)\n",
    "MLPmodel2.fit(X_train,Y_train)\n",
    "\n",
    "#Accuracy, Confusion Matrix, Precision, and Recall\n",
    "print(\"The Accuracy of the MLP Classifier is: \", MLPmodel2.score(X_test,Y_test))\n",
    "PredictedOutput=MLPmodel.predict(X_test)\n",
    "confusion=confusion_matrix(Y_test,PredictedOutput)\n",
    "print(\"The Confusion Matrix is: \\n\",confusion)\n",
    "print(\"The Precision score for MLP model is: \", precision_score(Y_test,PredictedOutput,average='macro'))\n",
    "print(\"The Recall score for MLP model is: \", recall_score(Y_test,PredictedOutput,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-3 </h4>\n",
    "- Repeat **Problem #4-2** but with model that use L-BFGS (a numerical quasi-Newton method) instead of stochastic gradient descent to find the weights.\n",
    "- Report accuracy, confusion matrix, precision, and recall\n",
    "- Comment on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the MLP Classifier is:  0.973684210526\n",
      "The Confusion Matrix is: \n",
      " [[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n",
      "The Precision score for MLP model is:  0.966666666667\n",
      "The Recall score for MLP model is:  0.979166666667\n"
     ]
    }
   ],
   "source": [
    "#Using L-BFGS(quasi Newton Method) to find the weights\n",
    "MLPmodel3=MLPClassifier(solver = 'lbfgs', activation = 'relu', random_state = 0, hidden_layer_sizes=[10,5], alpha = 0.5)\n",
    "MLPmodel3.fit(X_train,Y_train)\n",
    "\n",
    "#Accuracy, Confusion Matrix, Precision, and Recall\n",
    "print(\"The Accuracy of the MLP Classifier is: \", MLPmodel3.score(X_test,Y_test))\n",
    "PredictedOutput=MLPmodel.predict(X_test)\n",
    "confusion=confusion_matrix(Y_test,PredictedOutput)\n",
    "print(\"The Confusion Matrix is: \\n\",confusion)\n",
    "print(\"The Precision score for MLP model is: \", precision_score(Y_test,PredictedOutput,average='macro'))\n",
    "print(\"The Recall score for MLP model is: \", recall_score(Y_test,PredictedOutput,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER\n",
    "\n",
    "    The L-BFGS had a higher accuracy compared to Stochastic gradient descent. The Gradient descent model failed to converge and therefore the max iterations had to be increased. Increasing the random state or initializing the weights actually resulted in a lower accuracy among the gradient descent algorithms.\n",
    "    \n",
    "    The L-BFGS model had a better prediction accuracy. This suggests that the dataset is small and therefore it predicted better than the Gradient Descent. The Precision and Recall scores for both L-BFGS and Stochastic Gradient Descent were high and equal. This suggests that the data is not skewed and is accurately predicted by both the algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
